{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Detection\n",
    "This file provides an example of using RoBERTa classifier. The demonstration conducts binary classification on the G2D dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.roberta.tokenization_roberta import RobertaTokenizer\n",
    "from roberta.model import RoBERTClassifier\n",
    "from roberta.dataloader import DataManager, en_binary_labels, id2label_binary\n",
    "from roberta.trainer import SupervisedTrainer\n",
    "from utils import append_single_utterances, append_progressing_utterances\n",
    "import torch\n",
    "\n",
    "model_name = 'roberta-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset_path = os.path.abspath('../dataset')\n",
    "if dataset_path not in sys.path:\n",
    "    sys.path.append(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# human dataset = Missing Sentence Completion dataset\n",
    "human_dataset_gpt = pd.read_csv(os.path.join(dataset_path, 'Missing_Sentence_gpt.csv'), index_col=0).sort_values(by='dia_no', ascending=True, ignore_index=True)\n",
    "human_dataset_llama = pd.read_csv(os.path.join(dataset_path, 'Missing_Sentence_llama.csv'), index_col=0).sort_values(by='dia_no', ascending=True, ignore_index=True)\n",
    "human_dataset = pd.concat([human_dataset_gpt, human_dataset_llama], ignore_index=True).groupby('dia_no').sample(n=1).reset_index(drop=True)\n",
    "human_dataset['label'] = ['human']*human_dataset.shape[0]\n",
    "\n",
    "# ai dataset = llama and gpt G2D datasets\n",
    "llama_dataset = pd.read_csv(os.path.join(dataset_path, 'G2D_llama.csv'), index_col=0).sort_values(by='dia_no', ascending=True, ignore_index=True)\n",
    "llama_dataset = pd.DataFrame({\"dia_no\": llama_dataset['dia_no'], \n",
    "                              \"dia\": llama_dataset['dia'],\n",
    "                              \"label\":['ai']*llama_dataset.shape[0]})\n",
    "gpt_dataset = pd.read_csv(os.path.join(dataset_path, 'G2D_gpt.csv'), index_col=0).sort_values(by='dia_no', ascending=True, ignore_index=True)\n",
    "gpt_dataset = pd.DataFrame({\"dia_no\": gpt_dataset['dia_no'], \n",
    "                            \"dia\": gpt_dataset['dia'],\n",
    "                            \"label\":['ai']*gpt_dataset.shape[0]})\n",
    "\n",
    "# full dataset = concatenation of all classes\n",
    "dataset_df = pd.concat([human_dataset, llama_dataset, gpt_dataset], ignore_index=True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines to expand dataset by adding progressive / single utterances\n",
    "# dataset_df = append_single_utterances(dataset_df).reset_index(drop=True)\n",
    "# dataset_df = append_progressing_utterances(dataset_df).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: for next_response dataset, the name need to include \"next_response\" to ensure correct padding\n",
    "dataset_name = 'g2d_binary'\n",
    "\n",
    "print('Log INFO: initializing dataset...')\n",
    "data = DataManager(dataset_df, id2label_binary, 0.2, tokenizer, dataset_name, 16, load_from_cache=False)\n",
    "print(\"Log INFO: dataset initialization finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "print('-' * 32 + 'classify' + '-' * 32)\n",
    "classifier = RoBERTClassifier(model_name, id2label_binary)\n",
    "args = {\n",
    "     'num_train_epochs': 10,\n",
    "     'weight_decay': 0.1,\n",
    "     'lr': 1e-5,\n",
    "     'warm_up_ratio': 0.1\n",
    "}\n",
    "trainer = SupervisedTrainer(data, classifier, en_binary_labels, id2label_binary, args)\n",
    "ckpt_name = f'g2d_binary_roberta.pt'\n",
    "trainer.train(ckpt_name=ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and test the model\n",
    "saved_model = torch.load(os.path.join(\"trained_model\", ckpt_name))\n",
    "trainer.model.load_state_dict(saved_model.state_dict())\n",
    "trainer.test(data.test_dataloader, content_level_eval=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
